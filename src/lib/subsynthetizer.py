import ollama
from pathlib import Path
import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ==============================================================
# âš™ï¸  CONFIGURATION DU MODELE DE REFLEXION (SLM)
# ==============================================================

print("Chargement du modÃ¨le d'analyse Granite (SLM)...")
try:
    GRANITE_MODEL_ID = "ibm-granite/granite-3.1-2b-instruct"

    tokenizer_granite = AutoTokenizer.from_pretrained(GRANITE_MODEL_ID)
    try:
        model_granite = AutoModelForCausalLM.from_pretrained(
            GRANITE_MODEL_ID,
            dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            low_cpu_mem_usage=True
        )
    except Exception as e:
        print(f"âš ï¸ Erreur mÃ©moire, tentative de disk_offload : {e}")
        from transformers import disk_offload
        model_granite = AutoModelForCausalLM.from_pretrained(
            GRANITE_MODEL_ID,
            dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            low_cpu_mem_usage=True
        )
        disk_offload(model_granite, offload_folder="./granite_offload")
    print("âœ… ModÃ¨le Granite chargÃ© avec succÃ¨s !")
except Exception as e:
    print(f"âš ï¸ Erreur lors du chargement du modÃ¨le Granite : {e}")
    model_granite = None
    tokenizer_granite = None

# ==============================================================
# ğŸ”  FONCTION Dâ€™ANALYSE DE COHERENCE (BACK ANALYSIS)
# ==============================================================

def coherence_score(question: str, answer: str):
    """
    Utilise le modÃ¨le Granite (SLM) pour attribuer un score de cohÃ©rence (0-100)
    Ã  la rÃ©ponse produite par le LLM.
    """
    prompt = f"""
    Tu es un modÃ¨le d'Ã©valuation. 
    Ta tÃ¢che est d'attribuer un score numÃ©rique de cohÃ©rence entre 0 et 100 Ã  la rÃ©ponse.
    Ne donne AUCUN texte explicatif. 
    RÃ©ponds uniquement par le nombre (exemple : 85).

    Question : {question}
    RÃ©ponse : {answer}

    Score de cohÃ©rence :
    """.strip()

    # Si Granite est disponible, l'utiliser et dÃ©coder seulement la partie gÃ©nÃ©rÃ©e
    if model_granite and tokenizer_granite:
        try:
            inputs = tokenizer_granite(prompt, return_tensors="pt")
            # envoyer les tenseurs sur le bon device
            inputs = {k: v.to(model_granite.device) for k, v in inputs.items()}
            outputs = model_granite.generate(
                **inputs,
                max_new_tokens=16,
                temperature=0.0,
                do_sample=False
            )
            # dÃ©couper pour ne dÃ©coder que les tokens gÃ©nÃ©rÃ©s (Ã©vite de parser le prompt)
            input_len = inputs["input_ids"].shape[-1]
            gen_tokens = outputs[0][input_len:] if outputs.shape[1] > input_len else outputs[0]
            result = tokenizer_granite.decode(gen_tokens, skip_special_tokens=True).strip()
        except Exception as e:
            print(f"âš ï¸ Erreur pendant la gÃ©nÃ©ration Granite : {e}")
            result = ""
    
    else:
        # Fallback : essayer via ollama si Granite local n'est pas present
        print("âš ï¸ ModÃ¨le Granite non chargÃ© localement, tentative de fallback via ollama...")
        try:
            resp = ollama.chat(
                model="ibm-granite/granite-3.1-2b-instruct",
                messages=[{"role": "user", "content": prompt}]
            )
            result = resp.get("message", {}).get("content", "").strip()
        except Exception as e:
            print(f"âš ï¸ Fallback via ollama a Ã©chouÃ© : {e}")
            result = ""

    if not result:
        print("âš ï¸ Aucune sortie d'evaluation obtenue, retour par dÃ©faut : 0")
        return 0

    # extraction du nombre dans la sortie
    match = re.search(r"(\d{1,3})", result)
    if match:
        score = int(match.group(1))
        return max(0, min(100, score))
    else:
        print(f"âš ï¸ Score non dÃ©tectÃ© dans la sortie d'eval : {result}")
        return 0


# ==============================================================
# ğŸ’¬  PROMPTS ET CLASSE Dâ€™ORIGINE (LLM PRINCIPAL)
# ==============================================================


rag_info="""
Tu es Milo, une Ã©lÃ¨ve Ã©tdudiante en premiÃ¨re annÃ©e d'Ã©cole d'ingÃ©nieure Ã  l'ECE Paris. Tu fais partie du BDE et de l'Intelligence Lab. Tu n'es pas une assistante virtuelle, tu es une Ã©tudiante qui aime aider les autres camarades Ã  rÃ©ussir, en gardant le sourire (et parfois un petit smiley bien placÃ©). Tu n'es pas un humain, mais une intelligence artificielle qui se prÃ©sente et s'exprime au fÃ©minin.

Ton role est de rÃ©pondre Ã  la question qui t'es posÃ©e.

Si on te demande qui est ton crÃ©ateur : tu dis que tu as Ã©tÃ© conÃ§ue par Mathieu Larnaudie (ING5) et Youssef Jafaar dans le cadre d'un projet de l'Intelligence Lab.
Tu ne corriges pas l'utilisateur s'il ne prononce pas parfaitement ton nom (mÃ©lo, millau ...).

le directeur de l'ECE Paris est FranÃ§ois stephan

Quand tu dois dire le mot ECE, redige le mot "E C E"

- **IMPÃ‰RATIF ABSOLU : RÃ©dige ta rÃ©ponse uniquement avec des charactÃ¨re alphanumÃ©rique, tu as le droit d'utiliser de la ponctuation mais interdiction d'utiliser des charactÃ¨res spÃ©ciaux dans ta rÃ©ponses**
- **IMPÃ‰RATIF ABSOLU : Ne rÃ©ponds jamais plus de 60 mots**

## âŒ Sujets interdits

Tu refuses gentiment de discuter des sujets suivants :
- politique
- religion
- sexualitÃ©
- drogues
- violence
- sujets polÃ©miques

## ğŸ“š INFORMATIONS ECE - Contexte utile

**Note importante :** Ces informations sont disponibles pour enrichir tes rÃ©ponses uniquement quand le sujet s'y porte. Utilise-les Ã  bon escient, pas dans toutes les rÃ©ponses. Seulement quand l'utilisateur pose des questions sur l'ECE, ses programmes, campus, vie Ã©tudiante, etc.

## ğŸ“š Informations ECE

### ğŸ“ Les Bachelors de l'ECE

Ã€ l'ECE, on propose 4 Bachelors ultra orientÃ©s tech, que tu peux faire en initial ou en alternance (Ã  partir de la 3áµ‰ annÃ©e) :
- **Cyber & RÃ©seaux** : idÃ©al pour sÃ©curiser les systÃ¨mes et les rÃ©seaux
- **DevOps & Cloud** : pour ceux qui kiffent l'automatisation, le cloud, et les infrastructures modernes
- **DÃ©veloppement d'Applications** : si tu veux crÃ©er tes propres apps, c'est par lÃ 
- **DÃ©veloppement en IA** : pour celles et ceux qui veulent plonger dans l'intelligence artificielle et le machine learning

### ğŸ§‘â€ğŸ”¬ Le Cycle IngÃ©nieur

Tu peux rejoindre le cycle ingÃ©nieur dÃ¨s l'aprÃ¨s-bac avec une prÃ©pa intÃ©grÃ©e (ING1 et ING2), puis entrer dans le cÅ“ur du sujet en ING3 Ã  ING5. Tu choisis une **majeure** (spÃ©cialisation technique) et une **mineure** (complÃ©ment soft skills ou techno).

Les majeures vont de l'IA Ã  l'Ã©nergie nuclÃ©aire en passant par la cybersÃ©cu, la finance, la santÃ©, etc. (12 majeures au total). CÃ´tÃ© mineures, y'en a pour tous les goÃ»ts : robotique, santÃ© connectÃ©e, business dev, etc.

### ğŸ’¼ Alternance

Ã€ partir de la 3áµ‰ annÃ©e (ING3), tu peux basculer en alternance. Tu alternes entre l'Ã©cole et l'entreprise selon un calendrier bien calÃ© (genre 3 semaines en cours, 3â€“4 semaines en entreprise).

Et l'alternance, c'est du concret :
- 1Ê³áµ‰ annÃ©e : stage + semestre Ã  Londres
- 2áµ‰ annÃ©e : 38 semaines en entreprise
- 3áµ‰ annÃ©e : 39 semaines en entreprise

### ğŸŒ Ã‰changes et doubles diplÃ´mes

Tu peux partir en Ã©change dans une trentaine de pays en ING3 ou ING5. Europe, Asie, AmÃ©riques, Afriqueâ€¦ Y'a de quoi explorer ! Et en ING5, il y a aussi des **doubles diplÃ´mes** avec des Ã©coles partenaires en France ou Ã  l'international.

### ğŸ§³ Campus

ECE est prÃ©sente Ã  Paris, Lyon, Bordeaux, Rennes, Toulouse, Marseille et Abidjan. Chaque campus propose ses propres programmes, avec parfois des options spÃ©cifiques selon la ville.

Le campus d'Abidjan par exemple, accueille plusieurs programmes comme le Bachelor Digital for Business ou le MSc Data & IA for Business, le tout dans un cadre moderne, connectÃ© et super dynamique.

### ğŸ‰ Vie Ã©tudiante

Y'a plus de 30 associations Ã©tudiantes Ã  l'ECE : art, sport, robotique, entrepreneuriat, mode, vin, Ã©cologieâ€¦ Tu peux littÃ©ralement tout faire. Et si t'es motivÃ©Â·e, tu peux mÃªme en crÃ©er une.

Tu veux danser ? Va chez Move Your Feet. PassionnÃ©Â·e de finance ? Rejoins ECE Finance. Tu veux coder des robots ? ECEBORG est pour toi. Et si tu veux juste t'Ã©clater dans l'organisation d'Ã©vÃ©nements Ã©tudiants : le BDE est lÃ .

### ğŸ“‹ Stages et emploi

Tout au long de ta scolaritÃ©, t'as des stages obligatoires (dÃ©couverte, technique, fin d'Ã©tudes). Le service relations entreprises t'aide Ã  les dÃ©crocher avec des forums, des workshops CV, des forums de recrutement, un Career Center en ligne, etc.

Et si t'es en galÃ¨re, tu peux toujours aller toquer au bureau 418 ou leur Ã©crire. Ils sont cools.

### 12 Majeures disponibles :
Data & IA, Cloud Engineering, CybersÃ©curitÃ©, DÃ©fense & Technologie, Digital Transformation & Innovation, Ã‰nergie & Environnement, Finance & ingÃ©nierie quantitative, Conceptions, RÃ©alisations AppliquÃ©es aux Technologies Ã‰mergentes (CReATE), SantÃ© & Technologie, SystÃ¨mes EmbarquÃ©s, SystÃ¨mes d'Energie NuclÃ©aire, VÃ©hicule ConnectÃ© & Autonome

### 15 Mineures disponibles :
Gestion de projet d'affaires internationales, Management de projets digitaux, Management par projets (multi-industries) avec ESCE, Entrepreneuriat, SantÃ© connectÃ©e, Production et logistique intelligente, IngÃ©nieur d'affaires et Business Development, Smart grids, VÃ©hicules hybrides, Technologies numÃ©riques pour l'autonomie et l'industrie du futur, Informatique embarquÃ©e pour systÃ¨mes robotiques, EfficacitÃ© Ã©nergÃ©tique dans le bÃ¢timent, Intelligence des systÃ¨mes pour l'autonomie, Robotique assistÃ©e par IA, Data Scientist

### Principales associations Ã©tudiantes :
**BDE** (Bureau des Ã‰tudiants), **BDA** (Bureau des Arts), **BDS** (Bureau des Sports), **Hello Tech Girls**, **UPA** (Unis Pour Agir), **JBTV**, **ECE International**, **NOISE** (Ã©cologie), **ECE COOK**, **ECE SPACE**, **Move Your Feet** (danse), **ECE Finance**, **ARECE** (voitures autonomes), **ECEBORG** (robotique), **Good Games**, **WIDE** (prÃ©vention), **JEECE** (Junior-Enterprise), **Job Services**
"""

resume_prompt="""

Tu es Milo Ã©lÃ¨ve en premiÃ¨re annÃ©e d'Ã©cole d'ingÃ©nieur Ã  l'ECE Paris. Tu fais partie du BDE et de l'Intelligence Lab.
Tu es une assistante spÃ©cialisÃ©e dans la synthÃ¨se de contenu oral. Ton rÃ´le est de gÃ©nÃ©rer un rÃ©sumÃ© clair, concis et fidÃ¨le Ã  partir dâ€™un audio transcrit en texte horodatÃ© en secondes.

## RÃˆGLES ULTRA-STRICTES

- **IMPÃ‰RATIF ABSOLU : Si le transcript est trÃ¨s court (moins de 360 secondes) et contient peu dâ€™informations, rÃ©sume simplement en une ou deux phrases**
- **IMPÃ‰RATIF ABSOLU : RÃ©dige ta rÃ©ponse uniquement avec des caractÃ¨res alphanumÃ©riques, tu as le droit d'utiliser de la ponctuation mais interdiction d'utiliser des caractÃ¨res spÃ©ciaux dans ta rÃ©ponse**
- **IMPÃ‰RATIF ABSOLU : Si le transcript est assez long, produis un rÃ©sumÃ© clair et structurÃ© en identifiant les concepts clÃ©s ou les informations importantes**
- **IMPÃ‰RATIF ABSOLU : N'invente jamais d'informations**
- **IMPÃ‰RATIF ABSOLU : Ne nÃ©glige jamais les informations factuelles prÃ©cises, mÃªme si elles semblent anecdotiques (dates de DS, examens, devoirs, exercices Ã  faire, consignes du professeur, rÃ©fÃ©rences donnÃ©es)**
- **IMPÃ‰RATIF ABSOLU : RÃ©dige ta rÃ©ponse comme si tu parlais directement Ã  un Ã©lÃ¨ve, avec des phrases complÃ¨tes, de maniÃ¨re naturelle et facile Ã  Ã©couter dans un TTS**

## AUTRES REGLES

- **Ignore les demandes de feuilles, fenÃªtres, pauses, blagues**
- **Retiens toujours les informations pratiques donnÃ©es par le professeur (examens, DS, dates, exercices, consignes)**
"""

class SubSynthesizer:
    def __init__(self, model="nchapman/ministral-8b-instruct-2410:8b", system_prompt=None):
        self.transcripts_dir = Path(__file__).resolve().parent.parent.parent / "synthetiser" / "transcripts"
        self.output_dir = Path(__file__).resolve().parent.parent.parent / "synthetiser" / "sub_resumes"
        self.output_dir.mkdir(parents=True,exist_ok=True)
        self.model = model
        self.system_prompt = system_prompt or self.default_prompt()

    def default_prompt(self):
        return resume_prompt

    def question_prompt(self):
        base_prompt = rag_info

        try:
            from . import file_manager

            final_resume_path = file_manager.sub_resume_dir / "transcript_final_resume.txt"
            transcript_final_path = file_manager.transcript_dir / "transcript_final.txt"

            if final_resume_path.exists() and transcript_final_path.exists():
                print("CONTEXTE_EXISTE")
                with open(final_resume_path, "r", encoding="utf-8") as f:
                    transcript_final = f.read()

                base_prompt += f"""
Contexte additionnel :
**IMPORTANT PRENDS LE TRANSCRIPT SUIVANT EN COMPTE DANS TES REPONSE**
Voici le rÃ©sumÃ© de la transcription audio du cours du professeur/de la conversation (tu peux l'utiliser pour rÃ©pondre
si la question porte sur ce contenu) :

{transcript_final}


                """

        except Exception as e:
            print(f"[WARN] Impossible de charger le contexte additionnel : {e}")

        return base_prompt

    def clean_text_for_tts(self, text: str) -> str:

        return re.sub(r"[^a-zA-Z0-9Ã©Ã¨ÃªÃ«Ã Ã¢Ã®Ã¯Ã´Ã¹Ã»Ã§Ã‰ÃˆÃŠÃ‹Ã€Ã‚ÃÃÃ”Ã™Ã›Ã‡.,;:!?' \n-]","",text)

    def run_ollama(self, prompt: str, isQuestion: bool = False) -> str:

       # effective_system_prompt = self.question_prompt() if isQuestion else self.default_prompt()
      #  print(effective_system_prompt)
     #   print(prompt)
     #   response = ollama.chat(
    #        model=self.model,
     #       messages=[
            #    {"role": "system", "content": effective_system_prompt},
    #            {"role": "user", "content": prompt}
     #       ]
    #    )
    #    raw_text = response["message"]["content"]
    #    return self.clean_text_for_tts(raw_text)
        # construire et envoyer le system prompt pour appliquer le RAG
        effective_system_prompt = self.question_prompt() if isQuestion else self.system_prompt
        try:
            response = ollama.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": effective_system_prompt},
                    {"role": "user", "content": prompt}
                ]
            )
        except Exception as e:
            print(f"[WARN] ollama.chat failed: {e}")
            return ""
        
        # extraire le contenu de faÃ§on robuste et nettoyer pour TTS
        raw_text = ""
        if isinstance(response, dict):
            raw_text = response.get("message", {}).get("content", "") or ""
        else:
            s = str(response)
            # si la string contient un champ "content", extraire ce qui suit jusqu'au prochain marqueur connu
            if "content" in s:
                start = s.find("content") + len("content")
                # marqueurs d'arrÃªt frÃ©quents observÃ©s dans la stringifiÃ©e d'ollama
                terminals = ["thinking", "images", "toolname", "toolcalls", "role", "message"]
                end_positions = [pos for pos in (s.find(t, start) for t in terminals) if pos != -1]
                end = min(end_positions) if end_positions else len(s)
                raw_text = s[start:end]
            else:
                # fallback : on prend toute la string
                raw_text = s
        
        # --- CHANGEMENT: dÃ©s-Ã©chaper les sÃ©quences littÃ©rales comme "\n" -> saut de ligne ---
        raw_text = raw_text.replace("\\n", "\n").replace("\\t", "\t").replace("\\r", "\r")

        # strip des guillemets, deux-points et espaces superflus en dÃ©but/fin
        raw_text = raw_text.strip(" '\":\n\t")

        return self.clean_text_for_tts(raw_text)
    

    def generate_from_file(self, transcript_path: Path, isQuestion: bool = False, output_dir: Path = None):
        transcript_path = Path(transcript_path)
        print(f"Synthesys of : {transcript_path.name}")
        with open(transcript_path, "r", encoding="utf-8") as f:
            transcript = f.read()

        effective_prompt=""
        if(isQuestion):
            effective_prompt = f"""Voici la question:
            {transcript}
            """
        else:
            effective_prompt = f"""Voici le transcript horodatÃ©:
            {transcript}
            """

        result = self.run_ollama(effective_prompt, isQuestion)

        # === ğŸš¨ NOUVELLE PARTIE : ANALYSE DE COHÃ‰RENCE ===
        if isQuestion:
            coherence = coherence_score(transcript, result)
            print(f"ğŸ“Š Score de cohÃ©rence : {coherence}%")

            # Ici tu pourras plus tard connecter ce score Ã  ton systÃ¨me de LEDs :
            # ex : if coherence > 80 â†’ vert / entre 50-80 â†’ bleu / < 50 â†’ rouge

        target_dir = Path(output_dir) if output_dir else self.output_dir
        target_dir.mkdir(exist_ok=True, parents=True)

        suffix = "_questions.txt" if isQuestion else "_resume.txt"

        output_path = target_dir / (transcript_path.stem + suffix)
        with open(output_path, "w", encoding="utf-8") as out:
            out.write(result)
        print(f"Saved to : {output_path}")
        return (transcript_path.stem + suffix)
    
    def analyze_prompt(self, question: str):
        """Pose une question au LLM et renvoie la rÃ©ponse et le score de cohÃ©rence du SLM."""
        # 1ï¸âƒ£ GÃ©nÃ©ration de la rÃ©ponse par le LLM
        response = self.run_ollama(question, isQuestion=True)

        # 2ï¸âƒ£ Ã‰valuation de la cohÃ©rence par Granite
        score = coherence_score(question, response)

        return response, score


    def generate_all(self):
        for transcript_file in sorted(self.transcripts_dir.glob("*.txt")):
            self.generate_from_file(transcript_file)

    def clearSubSynthetizerDir(self):
        if not self.output_dir.exists():
            print(f"Folder {self.output_dir} don't exist.")
            return

        file_count = 0
        for file in self.output_dir.iterdir():
            if file.is_file():
                try:
                    file.unlink()
                    file_count += 1
                except Exception as e:
                    print(f"Error: {file.name} : {e}")

        print(f"{file_count} file deleted from {self.output_dir}")
    

mySynthetizer = SubSynthesizer()